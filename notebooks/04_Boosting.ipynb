{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from src import load_data\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# Fonction pour calculer RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Fonction pour calculer MAPE\n",
    "def mape(y_true, y_pred):\n",
    "    return mean_absolute_percentage_error(y_true, y_pred) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('../data/processed/processed_data.csv')\n",
    "\n",
    "data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "data.set_index(\"date\",inplace=True)\n",
    "\n",
    "data.head()\n",
    "# Préparer les variables indépendantes et la cible\n",
    "#X= data.drop(columns=['Appliances', 'date'], axis=1)\n",
    "#y = data['Appliances']\n",
    "\n",
    "#X_test = data_test.drop(columns=['Appliances', 'date'], axis=1)\n",
    " #y_test = data_test['Appliances']\n",
    "train_index = (data.index <= \"2016-04-27 18:00:00\").sum()\n",
    "X_train = data.drop(columns=['Appliances'], axis=1).iloc[:train_index]\n",
    "y_train = data['Appliances'].iloc[:train_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des modèles\n",
    "xgb_model = XGBRegressor(n_estimators=150, learning_rate=0.05, max_depth=6, random_state=42)\n",
    "\n",
    "cart_model = DecisionTreeRegressor(max_depth=5, min_samples_split=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "cart_param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# 3. Configurer la Random Search\n",
    "cart_model = DecisionTreeRegressor()\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=cart_model,\n",
    "    param_distributions=cart_param_grid,\n",
    "    n_iter=20,  # Nombre d'itérations pour la recherche\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,  # Validation croisée à 3 plis\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. Ajustement du modèle\n",
    "random_search.fit(X_train, y_train)\n",
    "best_params_cart=random_search.best_params_\n",
    "best_params_cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def boosting_with_residuals(X, y, strong_model, train_index, total_steps, cart_max_depth=3):\n",
    "    \"\"\"\n",
    "    Implémente un modèle de boosting en ajoutant un apprenant final basé sur CART.\n",
    "    \n",
    "    Parameters:\n",
    "    - X : pd.DataFrame, variables explicatives.\n",
    "    - y : pd.Series, variable cible.\n",
    "    - strong_model : modèle fort préalablement entraîné.\n",
    "    - train_index : int, index de début pour les données d'entraînement.\n",
    "    - total_steps : int, nombre de pas à prédire à chaque itération.\n",
    "    - cart_max_depth : int, profondeur maximale du modèle CART.\n",
    "    \n",
    "    Returns:\n",
    "    - y_pred : Liste des prédictions combinées (strong_model + CART).\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    \n",
    "    for j in range(0, len(X) - train_index, total_steps):\n",
    "        # Entraînement du modèle fort sur les données actuelles\n",
    "        X_train, y_train = X.iloc[:train_index + j], y.iloc[:train_index + j]\n",
    "        X_test = X.iloc[train_index + j:train_index + j + total_steps]\n",
    "        y_test = y.iloc[train_index + j:train_index + j + total_steps]\n",
    "        \n",
    "        # Entraînement du modèle fort (e.g., un modèle préalablement entraîné comme XGBoost, etc.)\n",
    "        strong_model.fit(X_train, y_train)\n",
    "        y_pred_strong_train = strong_model.predict(X_train)\n",
    "        y_pred_strong_test = strong_model.predict(X_test)\n",
    "        \n",
    "        # Calcul des résidus (pondérés ou non)\n",
    "        residuals_train = y_train - y_pred_strong_train\n",
    "        residuals_test = y_test - y_pred_strong_test\n",
    "        \n",
    "        # Entraînement du modèle CART sur les résidus\n",
    "        cart_model = DecisionTreeRegressor(max_depth=cart_max_depth)\n",
    "        cart_model.fit(X_train, residuals_train)\n",
    "        residuals_pred_test = cart_model.predict(X_test)\n",
    "        \n",
    "        # Combinaison des prédictions\n",
    "        final_pred_test = y_pred_strong_test + residuals_pred_test\n",
    "        y_pred.extend(final_pred_test)\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def boosting_with_residuals(X, y, strong_model, train_index, total_steps,cart_max_depth=3):\n",
    "    \"\"\"\n",
    "    Implémente un modèle de boosting en ajoutant un apprenant final basé sur CART.\n",
    "    \n",
    "    Parameters:\n",
    "    - X : pd.DataFrame, variables explicatives.\n",
    "    - y : pd.Series, variable cible.\n",
    "    - strong_model : modèle fort préalablement entraîné.\n",
    "    - train_index : int, index de début pour les données d'entraînement.\n",
    "    - total_steps : int, nombre de pas à prédire à chaque itération.\n",
    "    - cart_max_depth : int, profondeur maximale du modèle CART.\n",
    "    \n",
    "    Returns:\n",
    "    - y_pred : Liste des prédictions combinées (strong_model + CART).\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    \n",
    "    for j in range(0, len(X) - train_index, total_steps):\n",
    "        # Entraînement du modèle fort sur les données actuelles\n",
    "        X_train, y_train = X.iloc[:train_index + j], y.iloc[:train_index + j]\n",
    "        X_test = X.iloc[train_index + j:train_index + j + total_steps]\n",
    "        y_test = y.iloc[train_index + j:train_index + j + total_steps]\n",
    "        \n",
    "        strong_model.fit(X_train, y_train)\n",
    "        y_pred_strong_train = strong_model.predict(X_train)\n",
    "        y_pred_strong_test = strong_model.predict(X_test)\n",
    "        \n",
    "        # Calcul des résidus (pondérés ou non)\n",
    "        residuals_train = y_train - y_pred_strong_train\n",
    "        residuals_test = y_test - y_pred_strong_test\n",
    "        \n",
    "        # Entraînement du modèle CART sur les résidus\n",
    "        cart_model = DecisionTreeRegressor(max_depth=3)\n",
    "        cart_model.fit(X_train, residuals_train)\n",
    "        residuals_pred_test = cart_model.predict(X_test)\n",
    "        \n",
    "        # Combinaison des prédictions\n",
    "        final_pred_test = y_pred_strong_test + residuals_pred_test\n",
    "        y_pred.extend(final_pred_test)\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = (data.index <= \"2016-04-27 18:00:00\").sum()\n",
    "X_train = data.drop(columns=['Appliances'], axis=1).iloc[:train_index]\n",
    "y_train = data['Appliances'].iloc[:train_index]\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 151, 10),  \n",
    "    'learning_rate': np.arange(0.01, 0.16, 0.01),  \n",
    "    'max_depth': np.arange(3, 11, 1),  \n",
    "    'min_child_weight': np.arange(1, 11, 1),  \n",
    "    'subsample': np.arange(0.5, 1.1, 0.1),  # Fraction des échantillons entre 0.5 et 1.0 avec un pas de 0.1\n",
    "    'colsample_bytree': np.arange(0.5, 1.1, 0.1),  # Fraction des colonnes par arbre entre 0.5 et 1.0\n",
    "    'gamma': np.arange(0, 0.51, 0.05),  # Paramètre gamma entre 0 et 0.5 avec un pas de 0.05\n",
    "    'scale_pos_weight': [1, 5, 10, 20],  \n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, n_iter=50, cv=3, scoring='neg_mean_squared_error', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Création du modèle avec les meilleurs paramètres\n",
    "optimized_model = XGBRegressor(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = random_search.best_params_\n",
    "\n",
    "# Création du modèle avec les meilleurs paramètres\n",
    "optimized_model = XGBRegressor(**best_params)\n",
    "y=data[\"Appliances\"]\n",
    "X=data.drop(columns=[\"Appliances\"],axis=1)\n",
    "X_test, y_test = data.iloc[train_index:], data['Appliances'].iloc[train_index:]\n",
    "#xgb_model = XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.1)\n",
    "\n",
    "# Boosting avec résidus\n",
    "y_pred = boosting_with_residuals(X ,y,optimized_model, train_index, 144)\n",
    "# Évaluation des performances (RMSE et MAPE)\n",
    "rmse_final = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mape_final = np.mean(np.abs((y_test- y_pred) / y_test)) * 100\n",
    "#xgb_model.fit(X_train,y_train)\n",
    "#rmse_bost=np.sqrt(mean_squared_error(y_test,xgb_model.predict(X_test) ))\n",
    "# Affichage des résultats d'évaluation\n",
    "#print(rmse_bost)\n",
    "print(f\"RMSE final : {rmse_final:.4f}\")\n",
    "print(f\"MAPE final : {mape_final:.4f}%\")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(y_test.index, y_test, label='Valeurs réelles', color='blue')\n",
    "plt.plot(y_test.index, y_pred, label='Prédictions Boosting', color='red')\n",
    "plt.legend()\n",
    "plt.title(\"Prédictions vs Réelles - Modèle Boosting\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Consommation (Appliances)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
